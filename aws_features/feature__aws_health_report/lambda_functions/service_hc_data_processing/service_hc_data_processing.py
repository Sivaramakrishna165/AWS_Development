"""
    The main purpose of this script is to process 
    all the CSV files that are generated by the 
    low level scripts using YAML configuration file.
    Input Example: {"S3_Bucket":"bucket-for-testing-221", "S3_directory_name":"feature_aws_health_checks/"}
"""

import os
import yaml
import csv
import operator
import re
import boto3
import json
import sys
import uuid
from datetime import datetime
from botocore.config import Config

config=Config(retries=dict(max_attempts=10,mode='standard'))
table_name = os.environ['table_name']
column_name = ''
condition = ''
threshold = ''

s3 = boto3.resource('s3', config=config)

'''
    This function will return the Line 
    number and the error that occured.
'''
def PrintException():
    exc_type, exc_obj, tb = sys.exc_info()
    f = tb.tb_frame
    lineno = tb.tb_lineno
    captureErr = "Line No. : " + str(lineno)  + " | ERROR: " + str(exc_obj)
    return captureErr


'''
    This function is updating the dynamoDB 
    table with the information send to it
'''
def put_data(ResourceName, TaskName, Result, Exception):

    print("hc_put_data called")
    now = datetime.now()
    #   hc_name = hc_name.replace(" ", "_")
    hc_name = os.environ['AWS_LAMBDA_FUNCTION_NAME']
    dynamodb_resource = boto3.resource('dynamodb',config=config)
    table = dynamodb_resource.Table(table_name)

    try:  
        table.update_item(
        Key={
            'AWS_HealthCheck_UUI': uniqueID},
        UpdateExpression= 'SET health_checks.#hc_name = list_append(health_checks.#hc_name, :obj)',
        ExpressionAttributeNames = { "#hc_name" : hc_name},
        ExpressionAttributeValues={
            ":obj": [
                    {
                        'ResourceName': ResourceName,
                        'TaskName': TaskName,
                        'Result': Result,
                        'Exception': Exception,
                        'Timestamp': now.strftime("%d/%m/%Y %H:%M:%S")
                    }
                ]}
        )
    except:
        print(PrintException())
        print("Error during table.update_item")

'''
    This function is creating 
    new record in dynamoDB 
'''
def hc_put_key(hc_name):

    print("hc_put_key called")
    dynamodb_resource = boto3.resource('dynamodb',config=config)
    table = dynamodb_resource.Table(table_name)
    print(f"Creating {hc_name}")
    try:

        response = table.update_item(
            Key={
                'AWS_HealthCheck_UUI': uniqueID},
                UpdateExpression=f'SET health_checks.#hc_name = :obj',
                ExpressionAttributeNames = { "#hc_name" : hc_name},
                ExpressionAttributeValues={":obj": []}
            )
    except:
        print(PrintException())
        print("Error during table.put_item")

'''
    This function is adding one more column to the 
    CSV it is reading and calling the yaml_processing_handler 
    function to determine the status and store it into the result 
    column.
'''
def data_processing(bucket_name, key, HC_name, local_file_uri):
    print("data_processing called")
    try:
        bucket = s3.Bucket(bucket_name)
        rows = []
        key_1 = key+ f'aws_health_check_output/{HC_name}.csv'
        print(key_1)
        filename = key_1.split('/')[-1]
        s3_object = s3.Object(bucket_name, key_1)                
        data = s3_object.get()['Body'].read().decode('utf-8').splitlines()
        fields = csv.reader(data)
        headers = next(fields)
        headers.append('Result')
        index = headers.index(column_name)
    except:
        Exception=PrintException()
        print(Exception)
        put_data("Data processing", "Adding Result column to the csv Report", "Error occurred while data processing ", Exception)
        print(f"{key_1 = }")
    else:
        for row in fields:   
            result = yaml_processing_handler(row[index])
            row.append(result)
            rows.append(row)            
        create_data_processed_report(local_file_uri + filename, headers, rows)
        upload_report(local_file_uri + filename, bucket_name, key+ 'aws_health_check_processed_output/'+ HC_name +".csv")

'''
    This function is reading the YAML Configuration 
    file from the S3 bucket and initializing the values 
    to the global variables.
'''
def yaml_file_reader(bucket_name, key):

    print("yaml_file_reader called")
    global column_name, condition, threshold

    # local_file_uri = R"C:\Users\akushwaha25\OneDrive - DXC Production\Desktop\\"
    local_file_uri = "/tmp/"
    yml_name = "service_health_check_config/"+'service_health_check_config.yml'
    
    try:
        response = boto3.client('s3', config=config).get_object(Bucket=bucket_name, Key= key+yml_name)
        configfile = yaml.safe_load(response["Body"])
        # print(configfile)
    except:
        Exception=PrintException()
        print(Exception)
        put_data("Configuration File", "Reading yaml configuration file from S3 bucket", "Error occurred while reading yaml configuration file", Exception)
    else:
        try:
            for DORM_Check_Name in configfile['DORM_Health_Checks']:

                for data in DORM_Check_Name['Sub_Checks']:

                    HC_name = data["Name"]
                    column_name = data['Column_Name']
                    condition = data['Condition']
                    threshold = data['Threshold']

                    print("HC_name: ",HC_name)
                    print("column: ",column_name)
                    print("condition: ",condition)
                    print("threshold: ",threshold)

                    data_processing(bucket_name, key, HC_name, local_file_uri)
        except:
            Exception=(PrintException())
            print(Exception)
            put_data("Configuration File", "Reading yaml configuration file from S3 bucket", "Failed to traverse", Exception)


'''
    This function takes the value and compares it 
    with the given condition (Global variable) and 
    returns the result weather it is PASS or FAIL.
'''
def yaml_processing_handler(value):

    if condition=="eq":
        result = 'PASS' if (str(value) == str(threshold)) else 'FAIL'

    elif condition=="le":
        try:
            result = 'PASS' if (float(value) <= float(threshold)) else 'FAIL'
        except:
            result = "Invalid Operands"

    elif condition=="ge":
        try:
            result = 'PASS' if (float(value) >= float(threshold)) else 'FAIL'
        except:
            result = "Invalid Operands"

    elif condition=="ne":
        result = 'PASS' if (str(value) != str(threshold)) else 'FAIL'

    elif condition=="gt":
        try:
            result = 'PASS' if (float(value) > float(threshold)) else 'FAIL'
        except:
            result = "Invalid Operands"

    elif condition=="lt":
        try:
            result = 'PASS' if (float(value) < float(threshold)) else 'FAIL'
        except:
            result = "Invalid Operands"

    elif condition=="nomatch":
       result = 'PASS' if re.match("(?!{})".format(threshold),str(value)) else 'FAIL'

    elif condition=="match":
       result = 'PASS' if re.match("{}".format(threshold),str(value)) else 'FAIL'
    
    else:
        result = "condition not found" 

    return result


'''
    This function takes file uri, fields and rows that needs 
    to be added and then creates the CSV file for it.
'''
def create_data_processed_report(filename, fields, rows):

    print("create_report called")
    try:
        with open(filename, 'w', newline='') as csvfile: 
            csvwriter = csv.writer(csvfile)
            csvwriter.writerow(fields)
            csvwriter.writerows(rows)
    except:
        Exception=PrintException()
        print(Exception)
        print("Error Occurred during Report Creation")
        put_data("Data processing report", "Creates CSV processed report", "Error Occurred during Report Creation ", Exception)
        
    else:
        print("Report Created Successfully")

'''
    This function upload the given file uri to 
    the S3 bucket and at particular key location provided.
'''
def upload_report(file_uri, bucket_name, key):
    print("upload_report called")
    try:
        s3 = boto3.resource('s3', config=config)
        s3.meta.client.upload_file(file_uri, bucket_name, key)
    except:
        print("File uri: ", file_uri)
        print("Key: ", key)
        Exception=PrintException()
        print(Exception)
        print("Error Occured during File Upload")
        put_data("S3 Bucket", "Upload processed CSV report to S3 bucket", "Error Occured during File Upload", Exception)
        
    else:
        print("File uploaded Successfully")
        print("File Saved at: ", key)
        
def token(event, task_token):

    sf = boto3.client('stepfunctions')
    sf_output = json.dumps(event)
    # task_token = event['token']

    sf_response = sf.send_task_success(
        taskToken=task_token,
        output=str(sf_output)
    )
    return sf_response


def lambda_handler(event, context):
    # TODO implement
    print("Received Event: ", event)
    global uniqueID
    try:
        task_token = event['token']
        event = event["Payload"]
        uniqueID = event["uniqueID"]
        hc_put_key(os.environ['AWS_LAMBDA_FUNCTION_NAME'])
        S3_directory_name = event["S3_directory_name"]+ "aws_service_health_check/"
        yaml_file_reader(event['S3_Bucket'], S3_directory_name)
        #return event
        return token(event, task_token)
    except:
        Exception=PrintException()
        print(Exception)
        print("Error Occured")
        put_data(f"{os.environ['AWS_LAMBDA_FUNCTION_NAME']} script", "", "Error Occured", Exception)

if __name__ == "__main__":
    event1 = {
                "S3_Bucket": "bucket-for-testing-221",
                "S3_directory_name": "feature_aws_health_checks/",
                "uniqueID": "healthCheckJobId_0d1fc1da-8026-11ed-96b1-d52a37cf75af"
            }   
    lambda_handler(event1, "")